{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milvus 向量数据库\n",
    "当前使用版本 <span style=\"color: red\">v2.2.11</span>\n",
    "\n",
    "首先需要在向量数据库中创建集合并添加索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import Collection, FieldSchema, DataType, CollectionSchema, connections\n",
    "\n",
    "connections.connect(alias='main', host=\"127.0.0.1\", port=19530)\n",
    "\n",
    "def create_index():\n",
    "    fields = [\n",
    "        FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "        FieldSchema(name=\"document_ebm\", dtype=DataType.FLOAT_VECTOR, dim=1024),\n",
    "        FieldSchema(name=\"document_text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "    ]\n",
    "    schema = CollectionSchema(fields=fields, description=\"database\")\n",
    "    # 创建集合\n",
    "    collection = Collection(name='documents', schema=schema, using='main')\n",
    "    # 添加索引\n",
    "    index_params = {\n",
    "        \"index_type\": \"HNSW\",\n",
    "        \"metric_type\": \"IP\",\n",
    "        \"params\": {\n",
    "            \"M\": 32,\n",
    "            \"efConstruction\": 512\n",
    "        }\n",
    "    }\n",
    "    collection.create_index(\n",
    "        field_name=\"document_ebm\",\n",
    "        index_params=index_params\n",
    "    )\n",
    "\n",
    "    collection.create_index(\n",
    "        field_name=\"document_text\",\n",
    "        index_params={\n",
    "            \"index_type\": \"marisa-trie\"\n",
    "        }\n",
    "    )\n",
    "    # 将数据加载到内存中\n",
    "    collection.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HNSW 索引类型\n",
    "HNSW 是一种基于图的索引算法。它按照一定的规则为图像构建多层导航结构。在这种结构中，上层更加稀疏，节点之间的距离更远；较低的层更密集，节点之间的距离更近。搜索从最上层开始，找到本层中距离目标最近的节点，然后进入下一层开始另一次搜索。经过多次迭代，可以快速逼近目标位置。\n",
    "\n",
    "为了提高性能，HNSW 将图每层节点的最大度数限制为 M 。此外，还可以使用 efConstruction （构建索引时）或 ef （搜索目标时）指定搜索范围。\n",
    "\n",
    "使用场景\n",
    "* 非常高速的查询\n",
    "* 要求召回率尽可能高\n",
    "* 大内存资源\n",
    "\n",
    "HNSW 索引params参数\n",
    "* M 节点的最大度数 (2, 2048)\n",
    "* efConstruction 索引时间内最近邻居的动态列表的大小。较高的 efConstruction 可能会提高索引质量，但代价是增加索引时间。(1, int_max)\n",
    "\n",
    "### IP （内积）\n",
    "两个嵌入之间的IP距离"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将文档转换成向量\n",
    "\n",
    "这里利用 langchain 框架的文档加载器来加载需要的文档资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "def loadDocumentation(path: str, glob: str = \"*.text\"):\n",
    "    \"\"\"读取目录下指定文件\"\"\"\n",
    "    loader = DirectoryLoader(path=path, glob=glob)\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用智谱AI向量模型将文本转换成Embedding，并将转换后的Embedding写入到Milvus 向量数据库中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhipuai import ZhipuAI\n",
    "from pymilvus import Collection, connections\n",
    "import os\n",
    "\n",
    "ZHIPUAI_API_KEY = os.environ.get('ZHIPUAI_API_KEY') # 替换成您自己的 key\n",
    "client = ZhipuAI(api_key=ZHIPUAI_API_KEY)\n",
    "\n",
    "connections.connect(alias='main', host=\"127.0.0.1\", port=19530)\n",
    "collection = Collection(name='documents', using='main')\n",
    "\n",
    "\n",
    "def documentEmbedding(docs):\n",
    "    document_ebm = []\n",
    "    document_text = []\n",
    "    for doc in docs:\n",
    "        embedding = getEmbedding(doc.page_content)\n",
    "        document_ebm.append(embedding)\n",
    "        document_text.append(doc.page_content)\n",
    "\n",
    "    documents = [document_ebm, document_text]\n",
    "    # 批量写入数据\n",
    "    res = collection.insert(data=documents)\n",
    "    collection.flush()\n",
    "    # 打印结果\n",
    "    print(f\"插入行数 -> {res.insert_count}\")\n",
    "\n",
    "\n",
    "def getEmbedding(text):\n",
    "    \"\"\" 调用智谱的 embedding-2 模型，将文本转换成 embedding 结果 \"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        model=\"embedding-2\",\n",
    "        input=text,\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 读取文档\n",
    "    docs = loadDocumentation(\"./text/\", \"*.text\")\n",
    "    documentEmbedding(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 根据提出的问题在向量库中查找最相似的结果\n",
    "Milvus 中的向量相似度搜索会计算查询向量与具有指定相似度度量的集合中的向量之间的距离，并返回最相似的结果。通过指定过滤标量字段或主键字段的布尔表达式，您可以执行混合搜索。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先将问题调用智谱的 embedding-2 模型，将问题转成 embedding，再利用问题的 embedding 去向量库中查询最相似的结果，返回 topk 1 个结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocuments(question: str):\n",
    "    query_vector = getEmbedding(question)\n",
    "    search_params = {\"metric_type\": \"IP\", \"params\": {\"nprobe\": 16}, \"offset\": 0}\n",
    "    results = collection.search(data=[query_vector], anns_field='document_ebm', param=search_params, limit=1, output_fields=['document_text'])\n",
    "    documents = []\n",
    "    for result in results:\n",
    "        for r in result:\n",
    "            document_text = str(r.entity.get('document_text'))\n",
    "            documents.append(document_text)\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langchain 已经集成了智谱的API接口，只需要导入 ChatZhipuAI \n",
    "\n",
    "定义AI的 prompt\n",
    "\n",
    "* System prompt \"\"\"请根据以下内容回答提出的问题，不要使用自身知识回答问题，回答的越详细越好\"\"\" AI 会根据自身的知识库来回答提出的问题，在 System prompt 中拒绝AI自身的回答\n",
    "* User prompt \"\"\"请基于以下内容:\n",
    "\\\"\\\"\\\"\n",
    "{context}\n",
    "\\\"\\\"\\\"\n",
    "来回答用户提出的问题，请用中文回答。\n",
    "\n",
    "问题: {question}\"\"\"\n",
    "\n",
    "使用 HumanMessagePromptTemplate 模版对象，传入 question 和 context，生成 user 的 prompt\n",
    "\n",
    "调用智谱的AI模型，得到流式输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import AsyncIteratorCallbackHandler\n",
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "from langchain_core.callbacks.manager import CallbackManager\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "async def sse_chat_bot(query: str):\n",
    "    documents = getDocuments(query)\n",
    "    print(\"查询到的文档\")\n",
    "    print(f\"\\033[92m{documents}\\033[0m\")\n",
    "    callback = AsyncIteratorCallbackHandler()\n",
    "    streaming_chat = ChatZhipuAI(\n",
    "        model=\"glm-3-turbo\",\n",
    "        api_key=ZHIPUAI_API_KEY,\n",
    "        temperature=0.01,\n",
    "        streaming=True,\n",
    "        callback_manager=CallbackManager([callback]),\n",
    "    )\n",
    "    # 定义问题模版\n",
    "    general_user_template = general_user_template = \"\"\"\n",
    "请基于以下内容:\n",
    "\\\"\\\"\\\"\n",
    "{context}\n",
    "\\\"\\\"\\\"\n",
    "来回答用户提出的问题，请用中文回答。\n",
    "\n",
    "问题: {question}\"\"\"\n",
    "\n",
    "    human_message = HumanMessagePromptTemplate.from_template(general_user_template, input_variables=[\"question\", \"context\"])\n",
    "    messages = [\n",
    "        SystemMessage(content=\"请根据以下内容回答提出的问题，不要使用自身知识回答问题，回答的越详细越好\"),\n",
    "        human_message.format(question=query, context='/n'.join(documents)),\n",
    "    ]\n",
    "\n",
    "    agenerate = streaming_chat.agenerate([messages])\n",
    "    finalAnswer = asyncio.create_task(agenerate)\n",
    "    async for token in callback.aiter():\n",
    "        yield json.dumps({\n",
    "            'output': token,\n",
    "            'end': False\n",
    "        }, ensure_ascii=False)\n",
    "\n",
    "    result = await finalAnswer\n",
    "\n",
    "    yield json.dumps({\n",
    "        'output': result.generations[0][0].text,\n",
    "        'end': True\n",
    "    }, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
