语言模型生成词元的过程是逐个进行的。可以把语言模型（具体来说，是仅解码器文本Transformer模型，但本文其余部分将其简称为LLM）理解为一个函数，它以词元作为输入并生成一个概率数组，用于表示词汇表中所有词元的概率（通常词汇表中有50-250K词元，每个词元由几个字母组成）。然后，程序根据这些概率从所有词元中进行采样，以指导采样过程，并生成下一个词元，这一过程会重复进行。这意味着，生成文本序列时不可能存在并行性——生成过程可以逐个词元地进行建模。
总的来说，语言模型在处理词元时会进行两种类型的操作：矩阵-向量乘法，其中一个大矩阵（例如8192x8192）与一个向量相乘，得到另一个向量，以及注意力计算。在生成过程中，模型不仅可以看到当前词元的状态，还可以看到序列中所有先前词元的内部状态，其中包括用户在提示中编写的词元以及模型生成的词元。这些状态存储在一个称为“KV-cache（键值缓存）”的结构中，它本质上是文本中每个先前位置的一组Key和Value向量。注意力机制会获取当前词元生成的query向量，计算它与所有先前位置的所有K向量的点积，然后对所得的一组标量进行归一化，并通过对所有先前位置的所有V向量进行加权求和得出一个V向量，使用点积作为注意力分数。
现在，矩阵-向量乘法和注意力计算都有一个重要的特点：对于从矩阵或KV缓存中读取的每个元素，我们需要执行少量的浮点运算。矩阵-向量乘法对每个矩阵元素执行一次乘加运算（2 FLOPs）；而注意力计算对每个Key元素进行一次乘加计算来计算点积，并对每个V元素进行一次乘加计算来计算加权和。
现代CPU和GPU进行ALU运算的速度（乘法、加法）远高于它们从内存读取输入的速度。例如：
AMD Ryzen 7950X的内存带宽为67 GB/s，浮点运算能力为2735 GFLOPS，FLOP:字节读取比为40:1。
NVidia GeForce RTX 4090的内存带宽为1008 GB/s，运算能力为83 TFLOPS，FLOP:字节读取比为82:1。
NVidia H100 SXM（一款数据中心显卡）的内存带宽为3350 GB/s，运算能力为67 TFLOPS，FLOP:字节读取比看似为20:1；然而，对于类似矩阵乘法的问题，张量核心提供了约494 TFLOPS的运算能力，因此FLOP:字节读取比为147:1，不考虑稀疏性。
对于较小的浮点数，如FP16或FP8，情况则变得更糟：H100张量核心在处理密集FP8矩阵时，理论吞吐量为1979 TFLOPS，这使得FLOP:字节读取比达到590:1。毋庸置疑，无论采用哪种配置，无论是否使用张量核心或使用何种浮点格式，ALU单元的资源都是充裕的。
因此，任何只需要对每个元素执行两次操作的问题必定会受限于带宽，我们应该能够通过模型配置、KV缓存大小以及可用带宽来估算运行推理过程所需的最短时间。